{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MGRS project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to obtain similar results as Sofia Antipolis\n",
    "- Rebuild the analysis from fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laboratory has developped cellular and animal tools to have a better understanding of intrinsic (plasmocyte) and extrinsic (kidney toxicity) impact of some immunoglobulins (Ig).\n",
    "Part 1 : Some anormal Igs (truncates or incompletes) are toxic for plasmocytes producing it. Factors of this toxicity will be study comparing plasmocytes transcriptome secreting normal vs anormal Ig.\n",
    "For each condition, 3 duplicates.\n",
    "Part 2 : We have shown that some Igs could remotly induce kidney disrupt and we want to analyse mechanisms leading to this toxicity. This second part is the 3rd biological replicate of an analysis already start with ReaIg 1922 project and ToxIg2. Analysis done on RNApolyA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Raw reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastq sent from Sofia-Antipolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "# Untar file\n",
    "tar -xvf sirac.tar;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Mus Musculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1) Reference genome & associated GFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded from the \"Genome Reference Consortium\", latest realease :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "url='ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/635/GCA_000001635.8_GRCm38.p6';\n",
    "axel -q $url/GCA_000001635.8_GRCm38.p6_genomic.fna.gz;\n",
    "axel -q $url/GCA_000001635.8_GRCm38.p6_genomic.gff.gz;\n",
    "unpigz *_GRCm38.p6_genomic.fna.gz;\n",
    "unpigz *_GRCm38.p6_genomic.gff.gz;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2) Reference transcriptome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Same question than PTCB :\n",
    "\n",
    "Which refererence transcriptome to use with Kallisto ? \n",
    "\n",
    "Asked question : I plan to work with Kallisto on the mus musculus transcriptome : Which DB should I use to retrieve the transcriptome ? : Genbank, Ensembl or UCSC ? \n",
    "\n",
    "Answer (Kallisto authors) : \"Anyway, our links would simply point you to the ensembl transcriptome when it exists. We typically use this as it is fairly complete (by comparison with others) and the transcript to gene name mapping is pretty nice (also by comparison with others).\" \n",
    "\n",
    "Thus we retrieve transcriptome from Ensembl latest release and use cdna + ncrna (to cover almost as many genes as the GENCODE annotation used with featureCounts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Ensembl release-91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "url='ftp://ftp.ensembl.org/pub/current_fasta/mus_musculus';\n",
    "axel -q $url/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz;\n",
    "axel -q $url/ncrna/Mus_musculus.GRCm38.ncrna.fa.gz;\n",
    "cat Mus_musculus.GRCm38.cdna.all.fa.gz \\\n",
    "    Mus_musculus.GRCm38.ncrna.fa.gz \\\n",
    "    > mm_ensembl_grcm38_r91_cdna_ncrna.fasta.gz;\n",
    "rm Mus_musculus.GRCm38.cdna.all.fa.gz Mus_musculus.GRCm38.ncrna.fa.gz;\n",
    "gunzip mm_ensembl_grcm38_r91_cdna_ncrna.fasta.gz;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3) GTF annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future use with featureCounts.\n",
    "\n",
    "Downloaded from \"GENCODE\". \n",
    "\n",
    "\"It contains the comprehensive gene annotation on the reference chromosomes, scaffolds, assembly patches and alternate loci (haplotypes)\". \n",
    "\n",
    "More precisely, it annotate a lot of gene/transcript biotypes : https://www.gencodegenes.org/gencode_biotypes.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "url='ftp://ftp.sanger.ac.uk/pub/gencode/Gencode_mouse/release_M16';\n",
    "axel -q $url/gencode.vM16.chr_patch_hapl_scaff.annotation.gtf.gz;\n",
    "unpigz *_scaff.annotation.gtf.gz;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove reference genome sequences if they are not in the annotation used for featureCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "python rename_fna_for_gtf_annotation.py -f ../../Data/MGRS/GCA_000001635.8_GRCm38.p6_genomic.fna -g ../../Data/MGRS/gencode.vM16.chr_patch_hapl_scaff.annotation.gtf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content of rename_fna_for_gtf_annotation.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "### A script allowing change header of fna (.fasta) file with gtf annotation\n",
    "\n",
    "\n",
    "def usage():\n",
    "\tprint('Usage:')\n",
    "\tprint('\\tpython '+sys.argv[0]+' -d <directory_of_fasta_files>')\n",
    "\tprint('\\t\\t-f or --fna : fna file')\n",
    "\tprint('\\t\\t-g or --gtf : gtf file')\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "\tfna_file = \"\"\n",
    "\tgtf_file = \"\"\n",
    "\t\t\n",
    "\ttry:\n",
    "\t\topts, args = getopt.getopt(sys.argv[1:], 'f:g:', ['fna=', 'gtf=', 'help'])\n",
    "\texcept getopt.GetoptError:\n",
    "\t\tusage()\n",
    "\t\tsys.exit(2)\n",
    "\n",
    "\tif not opts :\n",
    "\t\tusage()\n",
    "\t\tsys.exit(2)\n",
    "\tfor opt, arg in opts:\n",
    "\t\tif opt in ('-h', '--help'):\n",
    "\t\t\tusage()\n",
    "\t\t\tsys.exit(2)\n",
    "\t\telif opt in ('-f', '--fna'):\n",
    "\t\t\tfna_file = arg\n",
    "\t\telif opt in ('-g', '--gtf'):\n",
    "\t\t\tgtf_file = arg\n",
    "\t\telse:\n",
    "\t\t\tusage()\n",
    "\t\t\tsys.exit(2)\n",
    "\n",
    "\tif fna_file[-4:]!=\".fna\" or os.path.exists(fna_file)==False:\n",
    "\t\tprint(\"The fna file is missing or not .fna !\\n\")\n",
    "\t\tusage()\n",
    "\t\tsys.exit(2)\n",
    "\tif gtf_file[-4:]!=\".gtf\" or os.path.exists(gtf_file)==False:\n",
    "\t\tprint(\"The gtf file is missing or not .gtf !\\n\")\n",
    "\t\tusage()\n",
    "\t\tsys.exit(2)\n",
    "\t\n",
    "\toutput_rename = fna_file[:-4]+\"_renamed\"+\".fna\"\n",
    "\toutput_subsample = fna_file[:-4]+\"_subsampled\"+\".fna\"\n",
    "\n",
    "\tprint('\\n-----------------------------------------')\n",
    "\tprint('Fna file : '+fna_file)\n",
    "\tprint('GTF file : '+fna_file)\n",
    "\tprint('Output file rename: '+output_rename)\n",
    "\tprint('Output file rename: '+output_subsample)\n",
    "\tprint('-----------------------------------------\\n')\n",
    "\n",
    "\tlist_header_fna=[]\n",
    "\tlist_header_gtf=[]\n",
    "\tshared_header=[]\n",
    "\tf_output_rename = open(output_rename, 'a')\n",
    "\n",
    "\tdict_names={\"CM000994.2\":\"chr1\", \"CM000995.2\":\"chr2\", \"CM000996.2\":\"chr3\", \"CM000997.2\":\"chr4\", \"CM000998.2\":\"chr5\", \"CM000999.2\":\"chr6\", \"CM001000.2\":\"chr7\", \"CM001001.2\":\"chr8\", \"CM001002.2\":\"chr9\", \"CM001003.2\":\"chr10\", \"CM001004.2\":\"chr11\", \"CM001005.2\":\"chr12\", \"CM001006.2\":\"chr13\", \"CM001007.2\":\"chr14\", \"CM001008.2\":\"chr15\", \"CM001009.2\":\"chr16\", \"CM001010.2\":\"chr17\", \"CM001011.2\":\"chr18\", \"CM001012.2\":\"chr19\", \"CM001013.2\":\"chrX\", \"CM001014.2\":\"chrY\", \"AY172335.1\":\"chrM\", \"JH792831.2\":\"JH792831.1\", \"KQ030493.2\":\"KQ030493.1\"}\n",
    "\tfor record in SeqIO.parse(fna_file, 'fasta'):\n",
    "\t\tif record.id in dict_names:\n",
    "\t\t\trecord.id = dict_names[record.id]\n",
    "\t\t\tf_output_rename.write(\">\"+record.id+\"\\n\")\n",
    "\t\t\tf_output_rename.write(str(record.seq)+\"\\n\")\n",
    "\t\tlist_header_fna.append(record.id)\n",
    "\n",
    "\tf_gtf = open(gtf_file, \"r\")\n",
    "\tfor line in f_gtf:\n",
    "\t\tif not line.startswith('##'):\n",
    "\t\t\tif line.split('\\t')[0] not in list_header_gtf:\n",
    "\t\t\t\tlist_header_gtf.append(line.split('\\t')[0])\n",
    "\n",
    "\tcount_unique_fna=0\n",
    "\tcount_unique_gtf=0\n",
    "\tcount_both=0\n",
    "\n",
    "\tfor i in list_header_fna:\n",
    "\t\tif i in list_header_gtf:\n",
    "\t\t\tshared_header.append(i)\n",
    "\t\t\tcount_both+=1\n",
    "\t\telse:\n",
    "\t\t\tcount_unique_fna+=1\n",
    "\n",
    "\tfor i in list_header_gtf:\n",
    "\t\tif i not in list_header_fna:\n",
    "\t\t\tcount_unique_gtf+=1\n",
    "\n",
    "\tprint(\"Number of unique in fna : \"+str(count_unique_fna))\n",
    "\tprint(\"Number of unique in gtf : \"+str(count_unique_gtf))\n",
    "\tprint(\"Number of shared : \"+str(count_both))\n",
    "\n",
    "\tf_output_subsample = open(output_subsample, 'a')\n",
    "\n",
    "\tfor record in SeqIO.parse(output_rename, 'fasta'):\n",
    "\t\tif record.id in shared_header:\n",
    "\t\t\tf_output_subsample.write(\">\"+record.id+\"\\n\")\n",
    "\t\t\tf_output_subsample.write(str(record.seq)+\"\\n\")\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\tmain(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4) Ensembl vs GENCODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want at some point to compare results obtained with the \"genomic path\", i.e. STAR + featureCounts + DESeq2 vs results from the \"transcriptomic path\", i.e. Kallisto + DESeq2. Thus, we need make sure that featureCounts (using GENCODE annotations) and Kallisto (using transcriptomes sequences) will compute reads counts at the gene level for approximately the same genes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "python ensembl_vs_gencode.py -e ../../Data/MGRS/mm_ensembl_grcm38_r91_cdna_ncrna.fasta -g ../../Data/MGRS/gencode.vM16.chr_patch_hapl_scaff.annotation.gtf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content of ensembl_vs_gencode.py :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import os.path\n",
    "import shutil\n",
    "from Bio import SeqIO \n",
    "\n",
    "### A script allowing find shared annotation gencode/ensembl\n",
    "\n",
    "\n",
    "def usage():\n",
    "\tprint('Usage:')\n",
    "\tprint('\\tpython '+sys.argv[0]+' -e <ensembl file> -g <gencode file>')\n",
    "\tprint('\\t\\t-e or --ensembl : fasta file')\n",
    "\tprint('\\t\\t-g or --gencode : gtf file')\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "\tensembl_file = \"\"\n",
    "\tgencode_file = \"\"\n",
    "\t\t\n",
    "\ttry:\n",
    "\t\topts, args = getopt.getopt(sys.argv[1:], 'e:g:', ['ensembl=', 'gencode=', 'help'])\n",
    "\texcept getopt.GetoptError:\n",
    "\t\tusage()\n",
    "\t\tsys.exit(2)\n",
    "\n",
    "\tif not opts :\n",
    "\t\tusage()\n",
    "\t\tsys.exit(2)\n",
    "\tfor opt, arg in opts:\n",
    "\t\tif opt in ('-h', '--help'):\n",
    "\t\t\tusage()\n",
    "\t\t\tsys.exit(2)\n",
    "\t\telif opt in ('-e', '--ensembl'):\n",
    "\t\t\tensembl_file = arg\n",
    "\t\telif opt in ('-g', '--gencode'):\n",
    "\t\t\tgencode_file = arg\n",
    "\t\telse:\n",
    "\t\t\tusage()\n",
    "\t\t\tsys.exit(2)\n",
    "\n",
    "\tif ensembl_file[-6:]!=\".fasta\" or os.path.exists(ensembl_file)==False:\n",
    "\t\tprint(\"The ensembl file is missing or not .fasta !\\n\")\n",
    "\t\tusage()\n",
    "\t\tsys.exit(2)\n",
    "\tif gencode_file[-4:]!=\".gtf\" or os.path.exists(gencode_file)==False:\n",
    "\t\tprint(\"The gencode file is missing or not .gtf !\\n\")\n",
    "\t\tusage()\n",
    "\t\tsys.exit(2)\n",
    "\n",
    "\tprint('\\n-----------------------------------------')\n",
    "\tprint('Ensembl file : '+ensembl_file)\n",
    "\tprint('Gencode file : '+gencode_file)\n",
    "\tprint('-----------------------------------------\\n')\n",
    "\n",
    "\tannot_gencode=[]\n",
    "\tannot_ensembl=[]\n",
    "\tshared_annot=[]\n",
    "\n",
    "\tf_gencode_file = open(gencode_file, \"r\")\n",
    "\tfor line in f_gencode_file:\n",
    "\t\tgene_name=\"\"\n",
    "\t\tif not line.startswith('##'):\n",
    "\t\t\tgene_name=line.split('\\t')[8].split(\";\")[0].split(\"\\\"\")[1].split(\".\")[0]\n",
    "\t\t\t#print(gene_name)\n",
    "\t\t\tif gene_name not in annot_gencode:\n",
    "\t\t\t\tannot_gencode.append(gene_name)\n",
    "\n",
    "\tfor record in SeqIO.parse(ensembl_file, 'fasta'):\n",
    "\t\trecord.description = record.description.split(\" \")[3].split(\":\")[1].split(\".\")[0]\n",
    "\t\tif record.description not in annot_ensembl:\n",
    "\t\t\tannot_ensembl.append(record.description)\n",
    "\n",
    "\tcount_unique_ensembl=0\n",
    "\tcount_unique_gencode=0\n",
    "\tcount_both=0\n",
    "\n",
    "\tfor i in annot_gencode:\n",
    "\t\tif i in annot_ensembl:\n",
    "\t\t\tshared_annot.append(i)\n",
    "\t\t\tcount_both+=1\n",
    "\t\telse:\n",
    "\t\t\tcount_unique_gencode+=1\n",
    "\n",
    "\tfor i in annot_ensembl:\n",
    "\t\tif i not in annot_gencode:\n",
    "\t\t\tcount_unique_ensembl+=1\n",
    "\n",
    "\tprint(\"Number of unique in gencode : \"+str(count_unique_gencode))\n",
    "\tprint(\"Number of unique in ensembl : \"+str(count_unique_ensembl))\n",
    "\tprint(\"Number of shared : \"+str(count_both))\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\tmain(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique in gencode : 3122\n",
    "\n",
    "Number of unique in ensembl : 19\n",
    "\n",
    "Number of shared : 50805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50805 genes are common (94.87%), so we consider that we can compare DE results with counts coming from featureCounts and from Kallisto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Raw reads QC & filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) FastQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "fastqc /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2016.runs/161202_sirac_wtr_1003/*\n",
    "fastqc /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2017.runs/170411_Sirac_wtr_1024/170411_Sirac_wtr_1024_mouse/*\n",
    "fastqc /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2017.runs/171018_sirac_wtr_1069/*\n",
    "mkdir /media/sf_raid/Data/MGRS/FastQC\n",
    "mv /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2016.runs/161202_sirac_wtr_1003/*.zip /media/sf_raid/Data/MGRS/FastQC\n",
    "mv /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2016.runs/161202_sirac_wtr_1003/*.html /media/sf_raid/Data/MGRS/FastQC\n",
    "mv /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2017.runs/170411_Sirac_wtr_1024/170411_Sirac_wtr_1024_mouse/*.zip /media/sf_raid/Data/MGRS/FastQC\n",
    "mv /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2017.runs/170411_Sirac_wtr_1024/170411_Sirac_wtr_1024_mouse/*.html /media/sf_raid/Data/MGRS/FastQC\n",
    "mv /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2017.runs/171018_sirac_wtr_1069/*.zip /media/sf_raid/Data/MGRS/FastQC\n",
    "mv /media/sf_raid/Data/MGRS/sirac/000-NEXTSEQ-2017.runs/171018_sirac_wtr_1069/*.html /media/sf_raid/Data/MGRS/FastQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) MultiQC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile FastqQC analysis for all the data, to highlight potential run effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "cd /media/sf_raid/Data/MGRS/FastQC\n",
    "multiqc -o 161202 *S7*.zip *S8*.zip *S9*.zip\n",
    "multiqc -o 170411 *S10*.zip *S11*.zip *S12*.zip\n",
    "multiqc -o 171018 *S13*.zip *S14*.zip *S15*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize QC results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "cd /media/sf_raid/Data/MGRS/FastQC\n",
    "chromium-browser 161202/multiqc_report.html;\n",
    "chromium-browser 170411/multiqc_report.html;\n",
    "chromium-browser 171018/multiqc_report.html;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Assign contigs based on subject taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this purpose, we are using blast+_2.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get taxdb for blast+ locally (useful to directly retrieve staxids when database = nt or nr) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### CODE ###\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/blast/db/taxdb.tar.gz;\n",
    "wget ftp://ftp.ncbi.nlm.nih.gov/blast/db/taxdb.tar.gz.md5;\n",
    "for f in *.md5;\n",
    "do tmp=$(md5sum -c $f);\n",
    "   if [[ $tmp == *\": OK\" ]];\n",
    "   then tar -zxvf  ${f%.*};\n",
    "        rm $f ${f%.*};\n",
    "   fi;\n",
    "done;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ps : Don't forget to move tax files in the same dir as your other databases (nt, nr, etc...).<br>\n",
    "And don't forget to add the BLASTDB variable to your .bashrc (export BLASTDB = 'your_path)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Blastn vs nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to blast assembled contigs vs nt and afterwards check if contigs matched vs a plant sequence, a non-plant sequence or nothing. It's one of the first step to \"filter\" the raw/draft transcriptome assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### CODE ###\n",
    "biscem='/home/erwann/Desktop/RTDG/BISCEm';\n",
    "cd $biscem/Data;\n",
    "blast='/home/erwann/Software/Blast_2.7.1/bin';\n",
    "db='/home/erwann/Software/ncbi-blast-2.5.0+/blastdb';\n",
    "for id in 'hess';\n",
    "do unpigz $id.fasta.gz;\n",
    "   $blast/blastn -query $id.fasta \\\n",
    "                 -db $db/nt \\\n",
    "                 -out $biscem/Output/$id'_vs_nt.tsv' \\\n",
    "                 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send \\\n",
    "                          evalue bitscore qlen slen saccver staxids sskingdoms sblastnames stitle\" \\\n",
    "                 -num_threads 8 \\\n",
    "                 -culling_limit 1;\n",
    "   pigz $id.fasta;\n",
    "done;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Parse blastn result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb : The idea on how to check if a contig matched to plant with the staxids field came from this post : https://www.biostars.org/p/163595/#163603. <br>\n",
    "\n",
    "What it does basically is :<br>\n",
    "\"staxids\" allows us to query the NCBI taxonomy database for the lineage of a taxon information with the tool eutils.<br>\n",
    "We can then parse this eutils xml result to check if kingdom <=> \"Viridiplantae\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a dummy example for staxids = 3357 (Pseudotsuga menziesii <=> Douglas) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3357@Pseudotsuga menziesii\tsuperkingdom@Eukaryota\tkingdom@Viridiplantae\tphylum@Streptophyta\torder@Pinales\tfamily@Pinaceae\tgenus@Pseudotsuga\n"
     ]
    }
   ],
   "source": [
    "#### CODE ###\n",
    "# Retrive \"RECOFGE\" + superkingdom, in this order : E_SK_R_E_C_O_F_G\n",
    "efetch -db taxonomy \\\n",
    "       -id 3357 \\\n",
    "       -format xml \\\n",
    "       | xtract -pattern Taxon \\\n",
    "                  -sep '@' \\\n",
    "                  -element TaxId,ScientificName \\\n",
    "                -division LineageEx \\\n",
    "                -group Taxon \\\n",
    "                  -if Rank -equals superkingdom \\\n",
    "                  -or Rank -equals kingdom \\\n",
    "                  -or Rank -equals phylum \\\n",
    "                  -or Rank -equals class \\\n",
    "                  -or Rank -equals order \\\n",
    "                  -or Rank -equals family \\\n",
    "                  -or Rank -equals genus \\\n",
    "                    -sep '@' \\\n",
    "                    -element Rank,ScientificName;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sort_plant_hit_vs_nt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### CODE ###\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "usage = '\\t --------\\n' \\\n",
    "        '\\t| usage  : python sort_plant_hit_vs_nt.py f1 f2\\n' \\\n",
    "        '\\t| input  : f1 = blastn.tsv (vs nt)\\n' \\\n",
    "        '\\t| input  : f2 = seqs.fasta (blastn queries)\\n' \\\n",
    "        '\\t| output : \"f2\"_1.fasta (plant_hit)\\n' \\\n",
    "        '\\t| output : \"f2\"_2.fasta (non_plant_hit)\\n' \\\n",
    "        '\\t| output : \"f2\"_3.fasta (no_hit)\\n' \\\n",
    "        '\\t --------'\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print(usage)\n",
    "    sys.exit()\n",
    "\n",
    "##############\n",
    "### Step 1 ###\n",
    "##############\n",
    "print('\\n\\tStep 1) Retrieve taxonomic infos for each staxids with efetch')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# For each line in TSV (f1), fill staxids_set\n",
    "staxids_set = set()\n",
    "with open(sys.argv[1], 'r') as tsv:\n",
    "    for row in tsv:\n",
    "        columns = row.split('\\t')\n",
    "        # Sometimes you have more than one staxids for an entry\n",
    "        staxids = columns[15].split(';')\n",
    "        for i in staxids:\n",
    "            staxids_set.add(i)\n",
    "\n",
    "# Use staxids_set as query with efetch & store result\n",
    "# Don't give more than let's say 500 entries at a time to avoid timeout\n",
    "staxids_li = list(staxids_set)\n",
    "staxids_sub_li = [staxids_li[x:x + 500]\n",
    "                  for x in range(0, len(staxids_li), 500)]\n",
    "efetch_li = []\n",
    "for item in staxids_sub_li:\n",
    "    staxids_input = ','.join(str(z) for z in item)\n",
    "    # Details about \"cmd\" : https://www.biostars.org/p/163595/#271497\n",
    "    cmd = ('efetch -db taxonomy -id ' + staxids_input + ' -format xml | xtract '\n",
    "           '-pattern Taxon -sep \\'@\\' -element TaxId,ScientificName -division '\n",
    "           'LineageEx -group Taxon -if Rank -equals superkingdom -or Rank '\n",
    "           '-equals kingdom -or Rank -equals phylum -or Rank -equals class'\n",
    "           ' -or Rank -equals order -or Rank -equals family -or Rank -equals'\n",
    "           ' genus -sep \\'@\\' -element Rank,ScientificName')\n",
    "    cmd_result = os.popen(cmd).read()\n",
    "    cmd_result_split = cmd_result.split('\\n')\n",
    "    for i in cmd_result_split:\n",
    "        efetch_li.append(i)\n",
    "\n",
    "# Create a dict associating key=staxid with value=list=tax_infos\n",
    "taxonomy_dic = {}\n",
    "for line in efetch_li:\n",
    "    field = line.split('\\t')\n",
    "    tax_ids = field[0].split('@')\n",
    "    # Sometimes more than one staxid is associated to an entry\n",
    "    # e.g. \"170850@3666@Cucurbita hybrid cultivar\"\n",
    "    for i in tax_ids[:-1]:\n",
    "        taxonomy_dic.setdefault(i, [None, None, None, None, None, None, None])\n",
    "        for item in field:\n",
    "            if 'superkingdom@' in item:\n",
    "                taxonomy_dic[i][0] = item.split('@')[-1]\n",
    "            elif 'kingdom@' in item:\n",
    "                taxonomy_dic[i][1] = item.split('@')[-1]\n",
    "            elif 'phylum@' in item:\n",
    "                taxonomy_dic[i][2] = item.split('@')[-1]\n",
    "            elif 'class@' in item:\n",
    "                taxonomy_dic[i][3] = item.split('@')[-1]\n",
    "            elif 'order@' in item:\n",
    "                taxonomy_dic[i][4] = item.split('@')[-1]\n",
    "            elif 'family@' in item:\n",
    "                taxonomy_dic[i][5] = item.split('@')[-1]\n",
    "            elif 'genus@' in item:\n",
    "                taxonomy_dic[i][6] = item.split('@')[-1]\n",
    "\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds\\n')\n",
    "\n",
    "##############\n",
    "### Step 2 ###\n",
    "##############\n",
    "print('\\tStep 2) Assign contigs best hit to plant or non-plant')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# Assign contigs best hits to plant or non-plant based on taxonomy_dic infos\n",
    "qseqid_set, viridi_hit_set, non_viridi_hit_set = set(), set(), set()\n",
    "with open(sys.argv[1], 'r') as tsv:\n",
    "    for row in tsv:\n",
    "        columns = row.split('\\t')\n",
    "        qseqid, staxids = columns[0], columns[15].split(';')[0]\n",
    "        # Check if we encounter qseqid for the first time <=> best hit\n",
    "        if not qseqid in qseqid_set:\n",
    "            if taxonomy_dic[staxids][1] == 'Viridiplantae':\n",
    "                viridi_hit_set.add(qseqid)\n",
    "            else:\n",
    "                non_viridi_hit_set.add(qseqid)\n",
    "        qseqid_set.add(qseqid)\n",
    "\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds\\n')\n",
    "\n",
    "##############\n",
    "### Step 3 ###\n",
    "##############\n",
    "print('\\tStep 3) Find contigs with no hits')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# Read initial FASTA (f2) & check intersection with viridi_hit_set & non_viridi_hit_set\n",
    "# We can deduce contigs with no hit from this intersection\n",
    "cpt = 0\n",
    "no_hit_set = set()\n",
    "with open(sys.argv[2], 'r') as fa:\n",
    "    for line in fa:\n",
    "        if line.startswith('>'):\n",
    "            cpt += 1\n",
    "            line = line.lstrip('>')\n",
    "            fields = line.split()\n",
    "            no_hit_set.add(fields[0])\n",
    "\n",
    "before_union = len(no_hit_set)\n",
    "no_hit_set = no_hit_set - viridi_hit_set\n",
    "no_hit_set = no_hit_set - non_viridi_hit_set\n",
    "\n",
    "print('\\t\\t- number of seqs (>) in FASTA : ' + str(cpt))\n",
    "print('\\t\\t- number of headers added to initial set is ' + str(before_union))\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds\\n')\n",
    "\n",
    "##############\n",
    "### Step 4 ###\n",
    "##############\n",
    "print('\\tStep 4) Create output files')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# Create input files (sequence IDs list) for seqtk\n",
    "file_2 = sys.argv[2].split('/')\n",
    "sample = file_2[-1].split('.')[0]\n",
    "with open(sample + '_plant_hit.temp', 'w') as out:\n",
    "    for item in viridi_hit_set:\n",
    "        out.write(item + \"\\n\")\n",
    "with open(sample + '_non_plant_hit.temp', 'w') as out:\n",
    "    for item in non_viridi_hit_set:\n",
    "        out.write(item + \"\\n\")\n",
    "with open(sample + '_no_hit.temp', 'w') as out:\n",
    "    for item in no_hit_set:\n",
    "        out.write(item + \"\\n\")\n",
    "\n",
    "# Create output files (plant_hit = 1, non-plant_hit = 2, no-hit = 3) with seqtk\n",
    "os.system('seqtk subseq ' + sys.argv[2] + ' ' + sample +\n",
    "          '_plant_hit.temp > ' + sample + '_1.fasta')\n",
    "os.system('seqtk subseq ' + sys.argv[2] + ' ' + sample +\n",
    "          '_non_plant_hit.temp > ' + sample + '_2.fasta')\n",
    "os.system('seqtk subseq ' + sys.argv[2] + ' ' + sample +\n",
    "          '_no_hit.temp > ' + sample + '_3.fasta')\n",
    "\n",
    "sum_seqs = int(len(viridi_hit_set)) + int(len(non_viridi_hit_set)) + int(len(no_hit_set))\n",
    "print('\\t\\t- number of seqs with plant_hit : ' + str(len(viridi_hit_set)))\n",
    "print('\\t\\t- number of seqs in non_plant_hit : ' + str(len(non_viridi_hit_set)))\n",
    "print('\\t\\t- number of seqs with no_hit : ' + str(len(no_hit_set)))\n",
    "print('\\t\\t- sum of the 3 above : ' + str(sum_seqs))\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds\\n')\n",
    "\n",
    "os.system('rm *.temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it does : <br>\n",
    "Take the blastn TVS file and the contig FASTA file as input.<br>\n",
    "Then check for each contig best hit if kingdom <=> \"Viridiplantae\".<br>\n",
    "If so, we consider the contig to have a plant hit (output file \"_1\").<br>\n",
    "Else, we consider the contig to have a non-plant hit (output file \"_2\").<br>\n",
    "Then, iterate other the contig file to collect contigs name & check intersection with contigs having plant / non-plant hits. This allow to determine contigs with no hits (output file \"_3\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the script :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tStep 1) Retrieve taxonomic infos for each staxids with efetch\n",
      "\t\t=> 35.252 seconds\n",
      "\n",
      "\tStep 2) Assign contigs best hit to plant or non-plant\n",
      "\t\t=> 0.534 seconds\n",
      "\n",
      "\tStep 3) Find contigs with no hits\n",
      "\t\t- number of seqs (>) in FASTA : 799102\n",
      "\t\t- number of headers added to initial set is 799102\n",
      "\t\t=> 3.132 seconds\n",
      "\n",
      "\tStep 4) Create output files\n",
      "\t\t- number of seqs with plant_hit : 98818\n",
      "\t\t- number of seqs in non_plant_hit : 92665\n",
      "\t\t- number of seqs with no_hit : 607619\n",
      "\t\t- sum of the 3 above : 799102\n",
      "\t\t=> 6.974 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### CODE ###\n",
    "biscem='/home/erwann/Desktop/RTDG/BISCEm';\n",
    "cd $biscem/Output;\n",
    "for id in 'trinity_cdhitest';\n",
    "do unpigz $biscem/Data/$id'_renamed.fasta.gz';\n",
    "   python $biscem/Git/sort_plant_hit_vs_nt.py $id'_renamed_vs_nt.tsv' $biscem/Data/$id'_renamed.fasta';\n",
    "   pigz $biscem/Data/$id'_renamed.fasta';\n",
    "done;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was done for each known reference transcriptome + UGMA assemblies. Then basic stats were computed on output FASTA files :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### CODE ###\n",
    "for f in  hess*.fasta;\n",
    "do echo $f; perl ../Script/assemblyStats.pl $f;\n",
    "done;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are compiled here : https://docs.google.com/spreadsheets/d/1hYWprws5gd2-W2vgMux2lVAtXx5TVm4IenuwZr6DcF4/edit#gid=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) Blastx vs nr | plant_refseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is especially useful for contigs which have previously failed to be assigned to \"Viridiplantae\" vs nt. <br>\n",
    "Blastx vs a protein subject database may highlight more informative contigs. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1) Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "biscem='/home/erwann/Desktop/RTDG/BISCEm';\n",
    "blast='/home/erwann/Software/Blast_2.7.1/bin';\n",
    "unpigz $biscem/Data/plant_ref_seq.faa.gz;\n",
    "$blast/makeblastdb -in $biscem/Data/plant_ref_seq.faa -input_type fasta -dbtype prot -out plant_ref_seq;\n",
    "pigz $biscem/Data/plant_ref_seq.faa;\n",
    "mv plant_* /home/erwann/Software/ncbi-blast-2.5.0+/blastdb;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch actual blastx (vs plant_refseq) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "# Launch blastx\n",
    "biscem='/home/erwann/Desktop/RTDG/BISCEm';\n",
    "blast='/home/erwann/Software/Blast_2.7.1/bin';\n",
    "db='/home/erwann/Software/ncbi-blast-2.5.0+/blastdb';\n",
    "\n",
    "cd $biscem/Output;\n",
    "for id in 'trinity_cdhitest_renamed_2';\n",
    "do $blast/blastx -query $id.fasta \\\n",
    "                 -db $db/plant_ref_seq \\\n",
    "                 -out $id'_vs_plant_refseq_prot.tsv' \\\n",
    "                 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart \\\n",
    "                          qend sstart send evalue bitscore qlen slen saccver\" \\\n",
    "                 -num_threads 8 \\\n",
    "                 -culling_limit 1;\n",
    "done;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4) Parse blastx result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sort_plant_hit_vs_plant_refseq_prot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CODE ###\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "usage = '\\t --------\\n' \\\n",
    "        '\\t| usage  : python sort_plant_hit_vs_plant_refseq_prot.py f1 f2\\n' \\\n",
    "        '\\t| input  : f1 = blast.tsv (vs plant_ref_seq)\\n' \\\n",
    "        '\\t| input  : f2 = seqs.fasta (blastx queries)\\n' \\\n",
    "        '\\t| indir  : prot.accession2taxid.gz (to map saccver to staxids)\\n' \\\n",
    "        '\\t| output : \"f2\"_1.fasta (plant_hit)\\n' \\\n",
    "        '\\t| output : \"f2\"_2.fasta (non_plant_hit = a control here, should be empty)\\n' \\\n",
    "        '\\t| output : \"f2\"_3.fasta (no_hit)\\n' \\\n",
    "        '\\t --------'\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print(usage)\n",
    "    sys.exit()\n",
    "\n",
    "##############\n",
    "### Step 1 ###\n",
    "##############\n",
    "print('\\n\\tStep 1) Map saccver to staxids')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "cpt = 0\n",
    "qseqid_set, saccver_set = set(), set()\n",
    "with open(sys.argv[1], 'r') as tsv:\n",
    "    for row in tsv:\n",
    "        cpt += 1\n",
    "        columns = row.split('\\t')\n",
    "        qseqid, saccver = columns[0].rstrip(), columns[14].rstrip()\n",
    "        # Get rid of accession number version\n",
    "        saccver = saccver.split('.')[0]\n",
    "        if not qseqid in qseqid_set:\n",
    "            saccver_set.add(saccver)\n",
    "        qseqid_set.add(qseqid)\n",
    "\n",
    "# Use saccver_set as local query against prot.accession2taxid\n",
    "# create saccver.temp as input for grep\n",
    "with open('saccver.temp', 'w') as temp:\n",
    "    for item in saccver_set:\n",
    "        temp.write(item.rstrip() + \"\\n\")\n",
    "\n",
    "# map saccver to staxids\n",
    "cmd = 'zfgrep -f saccver.temp ../Data/all_prot.accession2taxid.gz'\n",
    "cmd_result = os.popen(cmd).read()\n",
    "# First attempt\n",
    "# cmd = ('zgrep -f saccver.temp prot.accession2taxid.gz | awk \\'{print $2\\\"\\t\\\"$3}\\' $_')\n",
    "# grep on unzipped file is faster (3.42 vs 5.28), but file is too big to please me\n",
    "# cmd = 'fgrep -f saccver.temp all_prot.accession2taxid'\n",
    "\n",
    "# store grep result in a temp file (easier to parse)\n",
    "with open('grep.temp', 'w') as temp:\n",
    "    temp.write(cmd_result)\n",
    "\n",
    "# Finally, map saccver to staxids in a dic\n",
    "saccver_to_staxids_dic = {}\n",
    "with open('grep.temp', 'r') as grep:\n",
    "    for line in grep:\n",
    "        field = line.split('\\t')\n",
    "        saccver_to_staxids_dic.setdefault(field[0].rstrip(), field[2].rstrip())\n",
    "\n",
    "print('\\t\\t- TSV have ' + str(cpt) + ' lines, containing ' +\n",
    "      str(len(saccver_set)) + ' unique saccver')\n",
    "print('\\t\\t- grep result contains ' + str(len(saccver_set)) + ' lines')\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds\\n')\n",
    "\n",
    "##############\n",
    "### Step 2 ###\n",
    "##############\n",
    "print('\\tStep 2) Retrieve taxonomic infos for each staxids with efetch')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# Use staxids_set as query with efetch & store result\n",
    "# Don't give more than let's say 500 entries at a time to avoid timeout\n",
    "staxids_set = set()\n",
    "for k, v in saccver_to_staxids_dic.items():\n",
    "    staxids_set.add(v)\n",
    "\n",
    "staxids_li = list(staxids_set)\n",
    "staxids_sub_li = [staxids_li[x:x + 500]\n",
    "                  for x in range(0, len(staxids_li), 500)]\n",
    "efetch_li = []\n",
    "for item in staxids_sub_li:\n",
    "    staxids_input = ','.join(str(z) for z in item)\n",
    "    # Details about \"cmd\" : https://www.biostars.org/p/163595/#271497\n",
    "    cmd = ('efetch -db taxonomy -id ' + staxids_input + ' -format xml | xtract '\n",
    "           '-pattern Taxon -sep \\'@\\' -element TaxId,ScientificName -division '\n",
    "           'LineageEx -group Taxon -if Rank -equals superkingdom -or Rank '\n",
    "           '-equals kingdom -or Rank -equals phylum -or Rank -equals class'\n",
    "           ' -or Rank -equals order -or Rank -equals family -or Rank -equals'\n",
    "           ' genus -sep \\'@\\' -element Rank,ScientificName')\n",
    "    cmd_result = os.popen(cmd).read()\n",
    "    cmd_result_split = cmd_result.split('\\n')\n",
    "    for i in cmd_result_split:\n",
    "        efetch_li.append(i)\n",
    "# 257314@Lactobacillus johnsonii NCC\n",
    "# 533\\tsuperkingdom@Bacteria\\tphylum@Firmicutes\\tclass@Bacilli\\torder@Lactobacillales\\tfamily@Lactobacillaceae\\tgenus@Lactobacillus'\n",
    "\n",
    "# Create a dict associating key=staxid with value=list=tax_infos\n",
    "taxonomy_dic = {}\n",
    "for line in efetch_li:\n",
    "    field = line.split('\\t')\n",
    "    tax_ids = field[0].split('@')\n",
    "    # Sometimes more than one staxid is associated to an entry\n",
    "    # e.g. \"170850@3666@Cucurbita hybrid cultivar\"\n",
    "    for i in tax_ids[:-1]:\n",
    "        taxonomy_dic.setdefault(i, [None, None, None, None, None, None, None])\n",
    "        for item in field:\n",
    "            if 'superkingdom@' in item:\n",
    "                taxonomy_dic[i][0] = item.split('@')[-1]\n",
    "            elif 'kingdom@' in item:\n",
    "                taxonomy_dic[i][1] = item.split('@')[-1]\n",
    "            elif 'phylum@' in item:\n",
    "                taxonomy_dic[i][2] = item.split('@')[-1]\n",
    "            elif 'class@' in item:\n",
    "                taxonomy_dic[i][3] = item.split('@')[-1]\n",
    "            elif 'order@' in item:\n",
    "                taxonomy_dic[i][4] = item.split('@')[-1]\n",
    "            elif 'family@' in item:\n",
    "                taxonomy_dic[i][5] = item.split('@')[-1]\n",
    "            elif 'genus@' in item:\n",
    "                taxonomy_dic[i][6] = item.split('@')[-1]\n",
    "# '41840': ['Eukaryota', 'Viridiplantae', 'Streptophyta', 'Sphagnopsida', 'Sphagnales', 'Sphagnaceae', 'Sphagnum']\n",
    "\n",
    "print('\\t\\t- taxonomy_dic len is ' + str(len(taxonomy_dic)))\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds\\n')\n",
    "\n",
    "##############\n",
    "### Step 3 ###\n",
    "##############\n",
    "print('\\tStep 3) Assign contigs best hit to plant or non-plant')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# Assign contigs best hits to plant or non-plant based on taxonomy_dic infos\n",
    "qseqid_set, viridi_hit_set, non_viridi_hit_set = set(), set(), set()\n",
    "with open(sys.argv[1], 'r') as tsv:\n",
    "    for row in tsv:\n",
    "        columns = row.split('\\t')\n",
    "        qseqid, saccver = columns[0].rstrip(), columns[14].rstrip()\n",
    "        # Get rid of accession number version\n",
    "        saccver = saccver.split('.')[0]\n",
    "        # Check if we encounter qseqid for the first time <=> best hit\n",
    "        if not qseqid in qseqid_set:\n",
    "            if taxonomy_dic[saccver_to_staxids_dic[saccver]][1] == 'Viridiplantae':\n",
    "                viridi_hit_set.add(qseqid)\n",
    "            else:\n",
    "                non_viridi_hit_set.add(qseqid)\n",
    "        qseqid_set.add(qseqid)\n",
    "\n",
    "print('\\t\\t- qseqid_set len is ' + str(len(qseqid_set)))\n",
    "print('\\t\\t- viridi_hit_set len is ' + str(len(viridi_hit_set)))\n",
    "print('\\t\\t- non_viridi_hit_set len is ' + str(len(non_viridi_hit_set)))\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds\\n')\n",
    "\n",
    "##############\n",
    "### Step 4 ###\n",
    "##############\n",
    "print('\\tStep 4) Find contigs with no hits')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# Read initial FASTA (f2) & check intersection with viridi_hit_set & non_viridi_hit_set\n",
    "# We can deduce contigs with no hit from this intersection\n",
    "cpt = 0\n",
    "no_hit_set = set()\n",
    "with open(sys.argv[2], 'r') as fa:\n",
    "    for line in fa:\n",
    "        if line.startswith('>'):\n",
    "            cpt += 1\n",
    "            line = line.lstrip('>')\n",
    "            # if line in no_hit_set:\n",
    "            #    print(line)\n",
    "            # no_hit_set.add(line)\n",
    "            fields = line.split()\n",
    "            no_hit_set.add(fields[0])\n",
    "\n",
    "before_union = len(no_hit_set)\n",
    "no_hit_set = no_hit_set - viridi_hit_set\n",
    "no_hit_set = no_hit_set - non_viridi_hit_set\n",
    "\n",
    "print('\\t\\t- number of seqs (>) in FASTA : ' + str(cpt))\n",
    "print('\\t\\t- number of headers added to initial set is ' + str(before_union))\n",
    "print('\\t\\t- number of res in no_hit_set : ' + str(len(no_hit_set)))\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds\\n')\n",
    "\n",
    "\n",
    "##############\n",
    "### Step 5 ###\n",
    "##############\n",
    "print('\\tStep 5) Create output files')\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# Create input files (sequence IDs list) for seqtk\n",
    "file_2 = sys.argv[2].split('/')\n",
    "sample = file_2[-1].split('.')[0]\n",
    "with open(sample + '_plant_hit.temp', 'w') as out:\n",
    "    for item in viridi_hit_set:\n",
    "        out.write(item + \"\\n\")\n",
    "with open(sample + '_non_plant_hit.temp', 'w') as out:\n",
    "    for item in non_viridi_hit_set:\n",
    "        out.write(item + \"\\n\")\n",
    "with open(sample + '_no_hit.temp', 'w') as out:\n",
    "    for item in no_hit_set:\n",
    "        field = item.split()\n",
    "        # out.write(item)\n",
    "        out.write(field[0] + '\\n')\n",
    "\n",
    "# Create output files (plant_hit = 1, non-plant_hit = 2, no-hit = 3) with seqtk\n",
    "os.system('seqtk subseq ' + sys.argv[2] + ' ' + sample +\n",
    "          '_plant_hit.temp > ' + sample + '_1.fasta')\n",
    "os.system('seqtk subseq ' + sys.argv[2] + ' ' + sample +\n",
    "          '_non_plant_hit.temp > ' + sample + '_2.fasta')\n",
    "os.system('seqtk subseq ' + sys.argv[2] + ' ' + sample +\n",
    "          '_no_hit.temp > ' + sample + '_3.fasta')\n",
    "\n",
    "sum_seqs = int(len(viridi_hit_set)) + \\\n",
    "    int(len(non_viridi_hit_set)) + int(len(no_hit_set))\n",
    "print('\\t\\t- number of seqs with plant_hit : ' + str(len(viridi_hit_set)))\n",
    "print('\\t\\t- number of seqs in non_plant_hit : ' + str(len(non_viridi_hit_set)))\n",
    "print('\\t\\t- number of seqs with no_hit : ' + str(len(no_hit_set)))\n",
    "print('\\t\\t- sum of the 3 above : ' + str(sum_seqs) +\n",
    "      ', which should be equal to : ' + str(cpt))\n",
    "print('\\t\\t=> ' + str(round(timeit.default_timer() - t0, 3)) + ' seconds')\n",
    "\n",
    "os.system('rm *.temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need an extra step compared to sort_plant_hit_vs_nt.py : <br>\n",
    "We need to map saccver to staxids, via the \"accession2taxid\" file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the script :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tStep 1) Map saccver to staxids\n",
      "\t\t- TSV have 80119 lines, containing 46660 unique saccver\n",
      "\t\t- grep result contains 46660 lines\n",
      "\t\t=> 218.973 seconds\n",
      "\n",
      "\tStep 2) Retrieve taxonomic infos for each staxids with efetch\n",
      "\t\t- taxonomy_dic len is 437\n",
      "\t\t=> 3.488 seconds\n",
      "\n",
      "\tStep 3) Assign contigs best hit to plant or non-plant\n",
      "\t\t- qseqid_set len is 55967\n",
      "\t\t- viridi_hit_set len is 55967\n",
      "\t\t- non_viridi_hit_set len is 0\n",
      "\t\t=> 0.255 seconds\n",
      "\n",
      "\tStep 4) Find contigs with no hits\n",
      "\t\t- number of seqs (>) in FASTA : 92665\n",
      "\t\t- number of headers added to initial set is 92665\n",
      "\t\t- number of res in no_hit_set : 36698\n",
      "\t\t=> 0.171 seconds\n",
      "\n",
      "\tStep 5) Create output files\n",
      "\t\t- number of seqs with plant_hit : 55967\n",
      "\t\t- number of seqs in non_plant_hit : 0\n",
      "\t\t- number of seqs with no_hit : 36698\n",
      "\t\t- sum of the 3 above : 92665, which should be equal to : 92665\n",
      "\t\t=> 0.403 seconds\n"
     ]
    }
   ],
   "source": [
    "### CODE ###\n",
    "biscem='/home/erwann/Desktop/RTDG/BISCEm';\n",
    "cd $biscem/Output;\n",
    "for id in 'trinity_cdhitest_renamed_2';\n",
    "do python $biscem/Git/sort_plant_hit_vs_plant_refseq_prot.py $id'_vs_plant_refseq_prot.tsv' $id.fasta;\n",
    "done;"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
